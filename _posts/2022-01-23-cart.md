---
layout: post
title: "Classification and regression trees"
blurb: ""
img: ""
author: "Andy Jones"
categories: journal
tags: []
<!-- image: -->
---

$$\DeclareMathOperator*{\argmin}{arg\,min}$$
$$\DeclareMathOperator*{\argmax}{arg\,max}$$

<style>
.column {
  float: left;
  width: 30%;
  padding: 5px;
}

/* Clear floats after image containers */
.row::after {
  content: "";
  clear: both;
  display: table;
}
</style>

Classification and regression tree (CART) models are flexible, interpretable nonparametric approaches to supervised learning problems.

## Preliminaries

In what follows, we assume that our dataset consists of pairs $(\mathbf{x}, y)$, where $\mathbf{x}$ is a vector of predictors, and $y$ is a response variable. In the classification setting, we assume $y$ is a member of a discrete set $y \in \mathscr{C}$ where $\mathscr{C}$ is a set of class labels. In this post, we'll assume all classification problems are binary for simplicity; in other words $\mathscr{C} = \\{0, 1\\}$. In the regression setting, we assume $y$ is a scalar, $y \in \mathbb{R}$.

We denote a dataset of $n$ samples as $\\{(\mathbf{x}\_i, y_i)\\}\_{i=1}^n$.

## A motivating toy example

The central idea behind CART models is to split up the predictor space (the space in which the covariates $\mathbf{x}$ reside) into $R$ regions and assign predictions to each of these regions. We use the training data to define the boundaries of these regions. Then, given a test sample $\mathbf{x}^\star$, we can make a prediction $\widehat{y}$ by finding which region $\mathbf{x}^\star$ falls into.

To give a simple example, suppose we are performing a binary classification task with one-dimensional predictors, and our training dataset is as follows:

| $\mathbf{x}$ | $y$ |
| :---:        |    :----:   |
| 1.3      | 0       |
| 4.2      | 0       |
| 0.9      | 0       |
| 3.8      | 0       |
| -1.3     | 1       |
| 0.1      | 1       |
| -0.4     | 1       |
| 0.2      | 1       |

In this case, our goal in fitting a CART model is roughly to find a threshold $\tau$ such that splits the predictor space into two regions:

- One region $\\{x : x \leq \tau\\}$ where "most" of the corresponding response values belong to class $c$ (where $c$ is $0$ or $1$) in this case.
- Another region $\\{x : x > \tau\\}$ where "most" of the corresponding response values belong to class $1 - c$.

To see how this works, let's first plot the data:

<center>
<figure>
  <img src="/assets/cart_one_d_example.png">
  <figcaption><i></i></figcaption>
</figure>
</center>

In this simple example, our goal is to draw a vertical line such that the two classes of points are maximally segregated by this line. As we will explore in more detail below, there are several ways to quantitatively measure the quality of this segregation of points. 

For binary outcomes, one of the most popular ways is to find splits that minimize the overall entropy of the bins. Consider the animation below, where we consider a number of different thresholds $\tau$ (as indicated by the moving vertical line in the left panel). In the right panel, we show the entropy induced by splitting the points at a given value (the current entropy shown with a red dot).

<center>
<video style="width:100%; text-align:center; display:block; margin-top:50px;" autoplay loop>
<source src="/assets/cart_entropy_one_d_example.mp4" type="video/mp4">
</video>
<figcaption style="margin-bottom:50px;"><i></i></figcaption>
</center>

In this case, a perfect split of the data exists, under which the split achieves an entropy of $0$. Of course, this is highly unlikely to happen in practice, but demonstrates what a good split looks like for this toy example.

Now that we have an intuitive sense of our goal, let's turn to a more formal algorithm for fitting these trees.

## The basic algorithm

The central idea behind CART models is to recursively find partitions of the dataset, like we did above, which results in a tree of binary decisions. When the three is formed, each of these decisions is of the form, 

>"Does feature $j$ in sample $i$ belong to $\mathcal{X}\_L$ or $\mathcal{X}\_R$?"

Here $\mathcal{X}\_L$ and $\mathcal{X}\_R$ are two disjoint sets (corresponding to "left child" and "right child") where $\mathcal{X}\_L \cup \mathcal{X}\_R = \mathcal{X}$, and $\mathcal{X}$ is the predictor space.

The goal when fitting CART models is then to define the sets $\mathcal{X}\_L$ and $\mathcal{X}\_R$ at each level of the tree such that our predictive model achieves the best performance possible. In most cases, choosing these sets at each level of the tree will be equivalent to finding a threshold $\tau$ that partitions the predictor space such that $\mathcal{X}\_L = \\{\mathbf{x} : x^j \leq \tau \\}$ and $\mathcal{X}\_R = \\{\mathbf{x} : x^j > \tau \\}$, where $x^j$ is the $j$th feature of $\mathbf{x}$.

To identify the best possible split, we need a way to compare the quality of any two splits. This is typically done by computing a measure of "impurity" of the resulting splits. Intuitively, we can think of the impurity as the amount of mixing between samples with dissimilar response variables in each node. Conversely, we want to maximize the "purity" of each node so that samples with similar response values end up in the same branch of the tree.

Putting these steps together into an algorithm, we have the following:

1. Begin at the root node of the tree (containing the full dataset).
2. Until convergence:
    1. Compute the impurity induced by every possible partition of the data, where each partition is defined by a threshold $\tau$.
    2. Identify the threshold $\tau^\star$ with minimal impurity and create two children containing these subsets.
    3. Recurse on the children $(x_i, y_i)\_{\leq \tau^\star}$ and $(x_i, y_i)\_{> \tau^\star}$.

To make predictions for a test sample $\mathbf{x}^\star$, we then follow through the decisions in the tree. At the leaf node, we predict $\widehat{y}^\star$ to be the majority class in this terminal node.

Let's return to our one-dimensional exmaple to demonstrate how CART fitting works. Below is an animation that recursively finds the best split of the data (shown by the vertical lines). Once an optimal split is found, we fix that partition (shown in a gray line) and recurse on the remaining branches that still have mixed classes.

<center>
<video style="width:100%; text-align:center; display:block; margin-top:50px;" autoplay loop>
<source src="/assets/cart_entropy_one_d_example_nonlinear.mp4" type="video/mp4">
</video>
<figcaption style="margin-bottom:50px;"><i></i></figcaption>
</center>

## Time complexity

When the predictors are continuous, there are $p (n - 1)$ possible splits of the data, where $n$ is the number of samples and $p$ is the number of features. Recall that we can sort a feature list of length $n$ in time $O(n \log n)$. Doing this for each of the $p$ features then extends the time complexity to $O(p n \log n)$.

When the predictors are categorical with $k$ unique classes, there are $2^{k-1} - 1$ possible partitions.

Thus, fitting these models can be computationally demanding when the number of samples or number of classes is large. Some more recent work has developed faster methods that use approximations to find partitions that are close to optimal.

## Pruning

If we run the above algorithm exhaustively, we will eventually end up with enough splits that each training sample belongs to its own partition. However, allowing the tree to achieve this level of granularity in the predictor space can result in overfitting. Given a held-out test sample, if we were to predict with this overfit tree, it may be overly influenced by the particular training set. More technically, this is an instance if a bias-variance tradeoff. As we allow the tree to become more complex, we obtain lower error (bias) on the training set, but we also run the risk of overfitting to any given training set (variance).

One common way to combat the issue of overfitting in CART models is to perform a post-hoc pruning process in which leaves of the tree are removed. Removing leaves of the tree is effectively equivalent to re-combining parts of the predictor space that had been partitioned in the fitting process. A popular way of deciding which leaves to prune is to use a "validation" set of data samples, and iteratively remove leaf nodes, which will typically result in a decrease in the validation error. This process can be repeated until the validation error begins to increase again.

<!-- ## Classification

## Regression -->

### Connection to logistic regression

Recall that logistic regression uses a linear transformation of the predictors to optimally split the two classes of responses. Specifically, the logistic regression model tries to find a coefficient vector $\boldsymbol{\beta} \in \mathbb{R}^p$ and an intercept $\beta_0$. The linear predictor is then given by $\mathbf{x}^\top \boldsymbol{\beta}$, which can then be transformed to be in the range $(0, 1)$. This transformation is usually chosen to be the logistic function:

$$p_i = \frac{1}{1 + \exp(-\mathbf{x}^\top \boldsymbol{\beta} + \beta_0)}.$$

Intuitively, the coefficients $\boldsymbol{\beta}$ control how "steep" the logistic function is when discriminating between two classes. For instance, the animation below shows the logistic function for a range of values for $\boldsymbol{\beta}$ in a one-dimensional example. Typically classification is performed such that class $1$ is predicted when $p_i > 0.5$.

<center>
<video style="width:100%; text-align:center; display:block; margin-top:50px;" autoplay loop>
<source src="/assets/logistic_regression_slope_animation.mp4" type="video/mp4">
</video>
<figcaption style="margin-bottom:50px;"><i></i></figcaption>
</center>

As we can see, the curve becomes steeper and steeper for larger absolute values of the coefficient $\beta$. When $\beta \in (-\infty, \infty)$, we obtain a perfect step function.

On the other hand, the intercept $\beta_0$ controls the overall location of this logistic curve. In the animation below, we can see that the curve is translated left or right depending on the value of $\beta_0$.

<center>
<video style="width:100%; text-align:center; display:block; margin-top:50px;" autoplay loop>
<source src="/assets/logistic_regression_slope_animation_intercept.mp4" type="video/mp4">
</video>
<figcaption style="margin-bottom:50px;"><i></i></figcaption>
</center>

Thus, to frame CART models in terms of logistic regression, CART models are essentially fitting the intercept of a logistic regression model where the coefficient vector is fixed at $\infty$ or $-\infty$ (depending on which class is in which partition).


## References

- Breiman, Leo, et al. "Classification and regressiontrees, wadsworth statistics." Probability Series, Belmont, California: Wadsworth (1984).
- Mihaela van der Schaar's [notes on CART models](http://www.stats.ox.ac.uk/~flaxman/HT17_lecture13.pdf)
- Loh, Wei‐Yin. "Classification and regression trees." Wiley interdisciplinary reviews: data mining and knowledge discovery 1.1 (2011): 14-23.


